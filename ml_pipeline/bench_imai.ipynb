{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44198830",
   "metadata": {},
   "source": [
    "# Benchmark Vision Models on CIFake Dataset\n",
    "\n",
    "This notebook benchmarks convolutional and transformer-based networks defined in `vision_models.py` on the [CIFake dataset](https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "453cd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from pipelines_torch.benchmark import BenchmarkRunner\n",
    "from pipelines_torch.base import SimplePredictor\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from pipelines_torch.vision_models import MODEL_REGISTRY\n",
    "from utils.metrics import METRIC_REGISTRY\n",
    "\n",
    "DATA_DIR = Path('cifake_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c04b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images\n",
      "License(s): other\n",
      "Downloading cifake-real-and-ai-generated-synthetic-images.zip to cifake_data\n",
      "  0%|                                                | 0.00/105M [00:00<?, ?B/s]Downloading cifake-real-and-ai-generated-synthetic-images.zip to cifake_data\n",
      "  0%|                                                | 0.00/105M [00:00<?, ?B/s]\n",
      "100%|████████████████████████████████████████| 105M/105M [00:00<00:00, 2.21GB/s]\n",
      "\n",
      "100%|████████████████████████████████████████| 105M/105M [00:00<00:00, 2.21GB/s]\n"
     ]
    }
   ],
   "source": [
    "# Requires Kaggle API credentials available as environment variables\n",
    "# KAGGLE_USERNAME and KAGGLE_KEY. See https://www.kaggle.com/docs/api.\n",
    "if not DATA_DIR.exists():\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    !kaggle datasets download -d birdy654/cifake-real-and-ai-generated-synthetic-images -p $DATA_DIR --unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593545bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c8c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(DATA_DIR / 'train', transform=transform)\n",
    "X = torch.stack([img for img, _ in train_ds]).numpy()\n",
    "y = np.array(train_ds.targets)\n",
    "val_ds = datasets.ImageFolder(DATA_DIR / 'test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64)\n",
    "\n",
    "class_names = train_ds.classes\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502399de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model configs for BenchmarkRunner\n",
    "model_configs = []\n",
    "for name, model_class in MODEL_REGISTRY.items():\n",
    "    # Skip models that might require special handling\n",
    "    if name in ['qwen2_vl_qlora']:  # Skip complex models for now\n",
    "        continue\n",
    "    model_configs.append({\n",
    "        \"name\": name,\n",
    "        \"class\": model_class,\n",
    "        \"params\": {\"num_classes\": 2}\n",
    "    })\n",
    "\n",
    "# Set up metrics\n",
    "metrics = [METRIC_REGISTRY[\"accuracy\"], METRIC_REGISTRY[\"f1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de776ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark on 100000 samples with 2 classes\n",
      "Data shape: (100000, 3, 32, 32)\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Model: simple_cnn | Augmentation: none\n",
      "Class distribution: {np.int64(0): np.int64(50000), np.int64(1): np.int64(50000)}\n",
      "Computed class weights: {np.int64(0): np.float64(1.0), np.int64(1): np.float64(1.0)}\n",
      "Computed class weights: {np.int64(0): np.float64(1.0), np.int64(1): np.float64(1.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models:   0%|          | 0/6 [00:09<?, ?it/s]       \u001b[A\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunner.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m results_df = \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBenchmark Results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(results_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/food/ml_pipeline/pipelines_torch/benchmark.py:154\u001b[39m, in \u001b[36mBenchmarkRunner.run\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    142\u001b[39m     pipeline = GeneralPipelineSklearn(\n\u001b[32m    143\u001b[39m         model=model,\n\u001b[32m    144\u001b[39m         metrics=\u001b[38;5;28mself\u001b[39m.metrics,\n\u001b[32m   (...)\u001b[39m\u001b[32m    151\u001b[39m         k_folds=\u001b[38;5;28mself\u001b[39m.k_folds,\n\u001b[32m    152\u001b[39m     )\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# Train with complete data - pipeline will handle augmentations and splits internally\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m training_history = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Print class weights if available\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.task_type == \u001b[33m\"\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(pipeline, \u001b[33m'\u001b[39m\u001b[33mclass_weights\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m pipeline.class_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/food/ml_pipeline/pipelines_torch/base.py:880\u001b[39m, in \u001b[36mGeneralPipeline.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.augmentations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    876\u001b[39m     X_train, y_train = \u001b[38;5;28mself\u001b[39m.augmentations(X_train, y_train,\n\u001b[32m    877\u001b[39m                                         max_factor=\u001b[38;5;28mself\u001b[39m.max_factor,\n\u001b[32m    878\u001b[39m                                         random_state=\u001b[38;5;28mself\u001b[39m.random_state)\n\u001b[32m--> \u001b[39m\u001b[32m880\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_single_fit_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/food/ml_pipeline/pipelines_torch/base.py:909\u001b[39m, in \u001b[36mGeneralPipeline._single_fit_internal\u001b[39m\u001b[34m(self, X_train, y_train, X_val, y_val)\u001b[39m\n\u001b[32m    907\u001b[39m     outputs = outputs.squeeze()\n\u001b[32m    908\u001b[39m loss = loss_fn(outputs, yb)\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m    911\u001b[39m train_loss += loss.item() * Xb.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/projects/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/projects/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/projects/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Configure BenchmarkRunner\n",
    "runner = BenchmarkRunner(\n",
    "    model_configs=model_configs,\n",
    "    augmentations=[None],  # No augmentations for now\n",
    "    metrics=metrics,\n",
    "    task_type=\"classification\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    epochs=10,  # Reduced for faster testing\n",
    "    batch_size=64,\n",
    "    use_kfold=False,  # Single train/val split for speed\n",
    "    learning_rate=1e-3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Run benchmark\n",
    "print(f\"Running benchmark on {len(X)} samples with {num_classes} classes\")\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Using device: {runner.device}\")\n",
    "\n",
    "results_df = runner.run(X, y)\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac9e846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data shape: (20000, 3, 32, 32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SimpleCNN:\n\tMissing key(s) in state_dict: \"features.0.weight\", \"features.0.bias\", \"features.3.weight\", \"features.3.bias\", \"classifier.1.weight\", \"classifier.1.bias\", \"classifier.3.weight\", \"classifier.3.bias\". \n\tUnexpected key(s) in state_dict: \"model_state_dict\", \"optimizer_state_dict\", \"scheduler_state_dict\", \"class_weights\", \"task_type\", \"history\", \"epochs\", \"batch_size\", \"device\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip complex models for now\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Load trained model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclass\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mparams\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_start\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvision_weights\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m is_torch_model = \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m'\u001b[39m\u001b[33mparameters\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     24\u001b[39m pipeline = SimplePredictor(\n\u001b[32m     25\u001b[39m     model=model,\n\u001b[32m     26\u001b[39m     task_type=\u001b[33m'\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/food/ml_pipeline/utils/utils.py:37\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_class, model_name, params, path_start)\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     36\u001b[39m         state_dict = torch.load(path, map_location=torch.device(\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     39\u001b[39m     model.model = joblib.load(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/projects/lib/python3.11/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for SimpleCNN:\n\tMissing key(s) in state_dict: \"features.0.weight\", \"features.0.bias\", \"features.3.weight\", \"features.3.bias\", \"classifier.1.weight\", \"classifier.1.bias\", \"classifier.3.weight\", \"classifier.3.bias\". \n\tUnexpected key(s) in state_dict: \"model_state_dict\", \"optimizer_state_dict\", \"scheduler_state_dict\", \"class_weights\", \"task_type\", \"history\", \"epochs\", \"batch_size\", \"device\". "
     ]
    }
   ],
   "source": [
    "from utils.utils import save_model, save_metrics, load_model\n",
    "import pandas as pd\n",
    "RESULTS_DIR = \"results\"\n",
    "TEST_DATA_DIR = Path('cifake_data/test')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_ds = datasets.ImageFolder(TEST_DATA_DIR, transform=transform)\n",
    "X_test = torch.stack([img for img, _ in test_ds]).numpy()\n",
    "y_test = np.array(test_ds.targets) # True labels for evaluation\n",
    "print(f\"Test Data shape: {X_test.shape}\")\n",
    "# Evaluate each model on test set\n",
    "# Evaluate all models\n",
    "test_results = []\n",
    "for model_cfg in model_configs:\n",
    "    model_name = model_cfg['name']\n",
    "    if model_name == 'qwen2_vl_qlora':\n",
    "        continue  # Skip complex models for now\n",
    "    # Load trained model\n",
    "    model = load_model(model_cfg['class'], model_name, model_cfg['params'], path_start='vision_weights')\n",
    "    is_torch_model = hasattr(model, 'parameters')\n",
    "    pipeline = SimplePredictor(\n",
    "        model=model,\n",
    "        task_type='classification'\n",
    "    )\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # Collect metrics\n",
    "    result = {}\n",
    "    for metric in metrics:\n",
    "        metric_key = getattr(metric, 'name', None) or getattr(metric, '__name__', None) or str(metric)\n",
    "        result[metric_key] = metric(y_test, y_pred)\n",
    "    result['model'] = model_name\n",
    "    test_results.append(result)\n",
    "\n",
    "# Save test results\n",
    "test_results_df = pd.DataFrame(test_results)\n",
    "test_results_df.to_csv(os.path.join(RESULTS_DIR, \"vision_test_results.csv\"), index=False)\n",
    "print(\"Test results saved to\", os.path.join(RESULTS_DIR, \"vision_test_results.csv\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "test_results_df.set_index('model').plot(kind='bar', figsize=(12,6))\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Metric\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "test_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45788ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
